{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# -------------------------------\n",
    "# Load the Fashion MNIST CSV file\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"/kaggle/input/fashion-mnist-train-csv/fashion-mnist_train.csv\")\n",
    "\n",
    "# Group the dataset by label (0â€“9)\n",
    "grouped = df.groupby(\"label\")\n",
    "\n",
    "# Lists to hold splits per class\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "# -------------------------------\n",
    "# Split each class into 80% train and 20% test\n",
    "# -------------------------------\n",
    "for label, group in grouped:\n",
    "    train_split, test_split = train_test_split(\n",
    "        group,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "        stratify=None  # No further stratification needed\n",
    "    )\n",
    "    train_list.append(train_split)\n",
    "    test_list.append(test_split)\n",
    "\n",
    "# Concatenate all train and test splits\n",
    "train_df = pd.concat(train_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = pd.concat(test_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Visualize one example per class\n",
    "# -------------------------------\n",
    "examples = train_df.groupby(\"label\").first().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(2, 5, i + 1)\n",
    "    img = examples.loc[i].drop(\"label\").values.astype(np.uint8).reshape(28, 28)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(f\"Label: {examples.loc[i, 'label']}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare training and testing data\n",
    "# -------------------------------\n",
    "X = train_df.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "y = train_df[\"label\"].values\n",
    "num_classes = np.max(y) + 1\n",
    "y = np.eye(num_classes)[y]  # One-hot encode labels\n",
    "\n",
    "X_test = test_df.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "y_test = test_df[\"label\"].values\n",
    "y_test = np.eye(num_classes)[y_test]\n",
    "\n",
    "# -------------------------------\n",
    "# Normalize inputs\n",
    "# -------------------------------\n",
    "np.random.seed(0)\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "print(X.shape, y.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Activation functions\n",
    "# -------------------------------\n",
    "def relu(x, grad):\n",
    "    if grad:\n",
    "        return (x > 0).astype(float)\n",
    "    else:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x, grad):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    if grad:\n",
    "        return s * (1 - s)\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def softmax(z, grad):\n",
    "    exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Define network architecture\n",
    "# -------------------------------\n",
    "arch = [300, 300, 10]   # Two hidden layers (300 neurons) + output layer (10)\n",
    "activations = [relu, relu, softmax]\n",
    "\n",
    "W = []   # Weights\n",
    "B = []   # Biases\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.002      # Learning rate\n",
    "batch = 32         # Mini-batch size\n",
    "best_test = 0      # Track best test accuracy\n",
    "lmbda = 0.001      # L2 regularization factor\n",
    "\n",
    "# -------------------------------\n",
    "# Data augmentation function\n",
    "# -------------------------------\n",
    "def augmentation(img):\n",
    "    # Random horizontal flip\n",
    "    if np.random.rand() < 0.5:\n",
    "        img = np.fliplr(img)\n",
    "    # Random rotation between -15 and +15 degrees\n",
    "    if np.random.rand() < 0.5:\n",
    "        angle = random.choice(np.arange(-15, 15))\n",
    "        M = cv2.getRotationMatrix2D((14, 14), angle, 1.0)\n",
    "        img = cv2.warpAffine(img, M, (28, 28), borderMode=cv2.BORDER_REFLECT)\n",
    "    return np.asarray(img)\n",
    "\n",
    "# -------------------------------\n",
    "# Label smoothing function\n",
    "# -------------------------------\n",
    "def label_smoothing(y_true, epsilon=0.1):\n",
    "    K = y_true.shape[1]\n",
    "    return (1 - epsilon) * y_true + epsilon / K\n",
    "\n",
    "# -------------------------------\n",
    "# He initialization of weights\n",
    "# -------------------------------\n",
    "for i in range(len(arch)):\n",
    "    if i == 0:\n",
    "        w = np.random.randn(X.shape[1], arch[i]) * np.sqrt(2. / X.shape[1])\n",
    "    else:\n",
    "        w = np.random.randn(arch[i - 1], arch[i]) * np.sqrt(2. / arch[i - 1])\n",
    "    b = np.zeros((1, arch[i]))\n",
    "    W.append(w)\n",
    "    B.append(b)\n",
    "\n",
    "# -------------------------------\n",
    "# Training loop over epochs\n",
    "# -------------------------------\n",
    "whole_accuracy = []\n",
    "whole_accuracy_test = []\n",
    "whole_cost = []\n",
    "whole_cost_test = []\n",
    "\n",
    "for e in range(600):\n",
    "    all_accuracy = []\n",
    "    all_cost = []\n",
    "    all_accuracy_test = []\n",
    "    all_cost_test = []\n",
    "\n",
    "    # -------------------------------\n",
    "    # Mini-batch training\n",
    "    # -------------------------------\n",
    "    for i in range(int(np.ceil(len(X) / batch))):\n",
    "        X_batch = X[batch * i : batch * (i + 1)]\n",
    "        y_batch = y[batch * i : batch * (i + 1)]\n",
    "        m_batch = X_batch.shape[0]\n",
    "\n",
    "        # Apply label smoothing\n",
    "        y_smooth = label_smoothing(y_batch, epsilon=0.1)\n",
    "\n",
    "        # Apply augmentation\n",
    "        X_image = X_batch.reshape(-1, 28, 28)\n",
    "        X_aug = np.array([augmentation(img) for img in X_image])\n",
    "        X_aug_flat = X_aug.reshape(-1, 784)\n",
    "\n",
    "        # Forward pass\n",
    "        A = X_aug_flat\n",
    "        all_A = []\n",
    "        all_Z = []\n",
    "        for j in range(len(W)):\n",
    "            Z = A @ W[j] + B[j]\n",
    "            A = activations[j](Z, grad=False)\n",
    "            all_A.append(A)\n",
    "            all_Z.append(Z)\n",
    "\n",
    "        # Compute cost with L2 regularization\n",
    "        cost = (-1 / m_batch) * np.sum(y_smooth * np.log(A + 1e-8))\n",
    "        cost += (lmbda / (2 * m_batch)) * sum([np.sum(w ** 2) for w in W])\n",
    "        all_cost.append(cost)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        y_pred = np.argmax(A, axis=1)\n",
    "        y_true = np.argmax(y_smooth, axis=1)\n",
    "        accuracy = np.mean(y_pred == y_true) * 100\n",
    "        all_accuracy.append(accuracy)\n",
    "\n",
    "        # Backpropagation\n",
    "        for j in reversed(range(len(W))):\n",
    "            if j == len(W) - 1:\n",
    "                dz = all_A[j] - y_smooth\n",
    "            else:\n",
    "                dz = (dz @ W[j + 1].T) * activations[j](all_Z[j], grad=True)\n",
    "\n",
    "            if j == 0:\n",
    "                dw = X_aug_flat.T @ dz\n",
    "            else:\n",
    "                dw = all_A[j - 1].T @ dz\n",
    "\n",
    "            W[j] -= (alpha / m_batch) * (dw + lmbda * W[j])\n",
    "            B[j] -= (alpha / m_batch) * np.sum(dz, axis=0, keepdims=True)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Evaluate on test set\n",
    "    # -------------------------------\n",
    "    for k in range(int(np.ceil(len(X_test) / batch))):\n",
    "        X_test_batch = X_test[batch * k : batch * (k + 1)]\n",
    "        y_test_batch = y_test[batch * k : batch * (k + 1)]\n",
    "        m_test = X_test_batch.shape[0]\n",
    "\n",
    "        # Forward pass\n",
    "        A = X_test_batch\n",
    "        for j in range(len(W)):\n",
    "            Z = A @ W[j] + B[j]\n",
    "            A = activations[j](Z, grad=False)\n",
    "\n",
    "        # Compute test cost\n",
    "        cost_test = (-1 / m_test) * np.sum(y_test_batch * np.log(A + 1e-8))\n",
    "        cost_test += (lmbda / (2 * m_test)) * sum([np.sum(w ** 2) for w in W])\n",
    "        all_cost_test.append(cost_test)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        y_pred_test = np.argmax(A, axis=1)\n",
    "        y_true_test = np.argmax(y_test_batch, axis=1)\n",
    "        accuracy_test = np.mean(y_pred_test == y_true_test) * 100\n",
    "        all_accuracy_test.append(accuracy_test)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Record epoch statistics\n",
    "    # -------------------------------\n",
    "    mean_acc_train = np.mean(all_accuracy)\n",
    "    mean_acc_test = np.mean(all_accuracy_test)\n",
    "    mean_cost_train = np.mean(all_cost)\n",
    "    mean_cost_test = np.mean(all_cost_test)\n",
    "\n",
    "    whole_accuracy.append(mean_acc_train)\n",
    "    whole_accuracy_test.append(mean_acc_test)\n",
    "    whole_cost.append(mean_cost_train)\n",
    "    whole_cost_test.append(mean_cost_test)\n",
    "\n",
    "    # Save model if test accuracy improves\n",
    "    if best_test < mean_acc_test:\n",
    "        best_test = mean_acc_test\n",
    "        for idx in range(len(W)):\n",
    "            np.save(f\"/kaggle/working/W{idx}.npy\", W[idx])\n",
    "            np.save(f\"/kaggle/working/B{idx}.npy\", B[idx])\n",
    "\n",
    "    # Print progress\n",
    "    print(\n",
    "        f\"Epoch {e} | Best Test Acc: {best_test:.2f}% | \"\n",
    "        f\"Train Acc: {mean_acc_train:.2f}% | Test Acc: {mean_acc_test:.2f}% | \"\n",
    "        f\"Train Cost: {mean_cost_train:.4f} | Test Cost: {mean_cost_test:.4f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
